{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"ImageCaptionwithRecognition.ipynb","provenance":[{"file_id":"1aE9zsPKzLKPpOw4PNIJRDerB_tLa_zEG","timestamp":1611235307614}],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.6"},"widgets":{"application/vnd.jupyter.widget-state+json":{"d76090fa28ec46c18d6619de89b21007":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_c6a8a8dd2d0a452bb45726317ce84d91","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_7b86dc97da4a4461bf766ed4f833292f","IPY_MODEL_b7c7251df4dc4f2ebb834090c4690347"]}},"c6a8a8dd2d0a452bb45726317ce84d91":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"7b86dc97da4a4461bf766ed4f833292f":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_059c390c7a024996b56b434e2dd25871","_dom_classes":[],"description":" 55%","_model_name":"FloatProgressModel","bar_style":"danger","max":20,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":11,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_ee11fb19751a4fd89cf169792232af82"}},"b7c7251df4dc4f2ebb834090c4690347":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_29936adb1ef54ad5a352cf8ee5626568","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 11/20 [1:40:23&lt;1:16:03, 507.01s/it]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_aa579e0c69034b4f856e2c99a7f2ad8e"}},"059c390c7a024996b56b434e2dd25871":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"ee11fb19751a4fd89cf169792232af82":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"29936adb1ef54ad5a352cf8ee5626568":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"aa579e0c69034b4f856e2c99a7f2ad8e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"783052fafaa843ea8a850a3901437b29":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_c75e2bfd3a124df6b5e889f0fc0f7046","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_540e72dc25944da8af8c2d582edddf84","IPY_MODEL_7441ecd69209422ab539aebbdb9262d0"]}},"c75e2bfd3a124df6b5e889f0fc0f7046":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"540e72dc25944da8af8c2d582edddf84":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_4159269a79b5488481a25a4666a25982","_dom_classes":[],"description":"100%","_model_name":"FloatProgressModel","bar_style":"success","max":108857766,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":108857766,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_81629f49d9df4a6092d1154e61bf9793"}},"7441ecd69209422ab539aebbdb9262d0":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_f2e46df31d324cc9b70bd5c9116f9b91","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 104M/104M [00:01&lt;00:00, 100MB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_85e4e6d6053342fa91f0dc6c922fbcea"}},"4159269a79b5488481a25a4666a25982":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"81629f49d9df4a6092d1154e61bf9793":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"f2e46df31d324cc9b70bd5c9116f9b91":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"85e4e6d6053342fa91f0dc6c922fbcea":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}}}}},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"uZ3qHCD-cCK8","executionInfo":{"status":"ok","timestamp":1618209116488,"user_tz":-180,"elapsed":39657,"user":{"displayName":"Лена","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiMK7DQoNpvJJ9b_baTOhOl-0KsCYViUs3aLi0N=s64","userId":"14206305893323667740"}},"outputId":"212aa0b1-c12c-44a4-fc55-70eb24cfd2d0"},"source":["from google.colab import drive\n","drive.mount('/content/drive')\n","%cd drive/My Drive/broutanlab_course/"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n","/content/drive/My Drive/broutanlab_course\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"hfht3kDDF2qE"},"source":["![](https://bloglunit.files.wordpress.com/2017/06/e18480e185b3e18485e185b5e186b71.png?w=740)"]},{"cell_type":"markdown","metadata":{"id":"1hKNCFRZF5zx"},"source":["Рассмотрим модель NIC, которая состоит из двух моделей CNN + RNN. CNN используется для извлечения фич из изображения, а RNN для генерации описания.\n","\n","Для обучения модели будем использовать датасет [MSCOCO](https://cocodataset.org/#download)."]},{"cell_type":"markdown","metadata":{"id":"UWjsR8K3GPWj"},"source":["Ноутбук основан на базе учебного проекта для [Deep Learning School (dlschool.org)](https://www.dlschool.org/)."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VQsBoejgccDy","executionInfo":{"status":"ok","timestamp":1618209125603,"user_tz":-180,"elapsed":1352,"user":{"displayName":"Лена","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiMK7DQoNpvJJ9b_baTOhOl-0KsCYViUs3aLi0N=s64","userId":"14206305893323667740"}},"outputId":"f24be287-229d-4141-d202-8e2f09fc72e6"},"source":["!nvidia-smi"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Mon Apr 12 06:32:05 2021       \n","+-----------------------------------------------------------------------------+\n","| NVIDIA-SMI 460.67       Driver Version: 460.32.03    CUDA Version: 11.2     |\n","|-------------------------------+----------------------+----------------------+\n","| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n","|                               |                      |               MIG M. |\n","|===============================+======================+======================|\n","|   0  Tesla K80           Off  | 00000000:00:04.0 Off |                    0 |\n","| N/A   50C    P8    30W / 149W |      0MiB / 11441MiB |      0%      Default |\n","|                               |                      |                  N/A |\n","+-------------------------------+----------------------+----------------------+\n","                                                                               \n","+-----------------------------------------------------------------------------+\n","| Processes:                                                                  |\n","|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n","|        ID   ID                                                   Usage      |\n","|=============================================================================|\n","|  No running processes found                                                 |\n","+-----------------------------------------------------------------------------+\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"gbeJNUwj4F9z","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1618209159805,"user_tz":-180,"elapsed":35330,"user":{"displayName":"Лена","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiMK7DQoNpvJJ9b_baTOhOl-0KsCYViUs3aLi0N=s64","userId":"14206305893323667740"}},"outputId":"95aac473-875e-4efe-fbf2-02389fcb4caf"},"source":["!pip install face_recognition"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Collecting face_recognition\n","  Downloading https://files.pythonhosted.org/packages/1e/95/f6c9330f54ab07bfa032bf3715c12455a381083125d8880c43cbe76bb3d0/face_recognition-1.3.0-py2.py3-none-any.whl\n","Requirement already satisfied: dlib>=19.7 in /usr/local/lib/python3.7/dist-packages (from face_recognition) (19.18.0)\n","Requirement already satisfied: Click>=6.0 in /usr/local/lib/python3.7/dist-packages (from face_recognition) (7.1.2)\n","Collecting face-recognition-models>=0.3.0\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/cf/3b/4fd8c534f6c0d1b80ce0973d01331525538045084c73c153ee6df20224cf/face_recognition_models-0.3.0.tar.gz (100.1MB)\n","\u001b[K     |████████████████████████████████| 100.2MB 40kB/s \n","\u001b[?25hRequirement already satisfied: Pillow in /usr/local/lib/python3.7/dist-packages (from face_recognition) (7.1.2)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from face_recognition) (1.19.5)\n","Building wheels for collected packages: face-recognition-models\n","  Building wheel for face-recognition-models (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for face-recognition-models: filename=face_recognition_models-0.3.0-py2.py3-none-any.whl size=100566173 sha256=f7ab0d9da911ae986a27f49158fe5459365184bfe9d7bdfe6c3781725caaad58\n","  Stored in directory: /root/.cache/pip/wheels/d2/99/18/59c6c8f01e39810415c0e63f5bede7d83dfb0ffc039865465f\n","Successfully built face-recognition-models\n","Installing collected packages: face-recognition-models, face-recognition\n","Successfully installed face-recognition-1.3.0 face-recognition-models-0.3.0\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"HW8rsoml4Ftw"},"source":["import face_recognition\n","from sklearn import svm\n","import os\n","from imutils import paths\n","import matplotlib.pyplot as plt\n","import cv2\n","from google.colab.patches import cv2_imshow"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"1TzYENKf4POL","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1618209315174,"user_tz":-180,"elapsed":190260,"user":{"displayName":"Лена","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiMK7DQoNpvJJ9b_baTOhOl-0KsCYViUs3aLi0N=s64","userId":"14206305893323667740"}},"outputId":"e08bc99e-e795-4aaa-bd20-b6eea6344c06"},"source":["# Training the SVC classifier\n","\n","# The training data would be all the face encodings from all the known images and the labels are their names\n","encodings = []\n","names = []\n","\n","# Training directory\n","train_dir = os.listdir('dataset4/')\n","\n","\n","# Loop through each person in the training directory\n","for person in train_dir:\n","    pix = os.listdir(\"dataset4/\" + person)\n","\n","    # Loop through each training image for the current person\n","    for person_img in pix:\n","        # Get the face encodings for the face in each image file\n","        face = face_recognition.load_image_file(\"dataset4/\" + person + \"/\" + person_img)\n","        face_bounding_boxes = face_recognition.face_locations(face)\n","\n","        #If training image contains exactly one face\n","        if len(face_bounding_boxes) == 1:\n","            face_enc = face_recognition.face_encodings(face)[0]\n","            # Add face encoding for current image with corresponding label (name) to the training data\n","            encodings.append(face_enc)\n","            names.append(person)\n","        else:\n","            print(person + \"/\" + person_img + \" was skipped and can't be used for training\")"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Selena Gomez/CANNES-2019-SELENA-GOMEZ-e1557932467454.jpg was skipped and can't be used for training\n","Leonardo Dicaprio/e436def2ea02e6f9b2dec5b88297e.jpg was skipped and can't be used for training\n","Ryan Gosling/Ryan-Gosling.jpg was skipped and can't be used for training\n","Ryan Gosling/Ryan-Gosling (1).jpg was skipped and can't be used for training\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"nEG6sXRT4PK6","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1618209315175,"user_tz":-180,"elapsed":190070,"user":{"displayName":"Лена","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiMK7DQoNpvJJ9b_baTOhOl-0KsCYViUs3aLi0N=s64","userId":"14206305893323667740"}},"outputId":"689f0f35-ba40-4ec5-fe84-0aefe71980e1"},"source":["# Create and train the SVC classifier\n","clf = svm.SVC(gamma='scale')\n","clf.fit(encodings,names)\n"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["SVC(C=1.0, break_ties=False, cache_size=200, class_weight=None, coef0=0.0,\n","    decision_function_shape='ovr', degree=3, gamma='scale', kernel='rbf',\n","    max_iter=-1, probability=False, random_state=None, shrinking=True,\n","    tol=0.001, verbose=False)"]},"metadata":{"tags":[]},"execution_count":6}]},{"cell_type":"code","metadata":{"id":"BsCh9VFr4ZMv"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"goT2U70hGH-L"},"source":["Скачаем данные."]},{"cell_type":"code","metadata":{"id":"Vq5yVA3wnDl-"},"source":["# создадим дирректорию, куда будем скачивать данные\n","!mkdir -p data\n","# скачаем данные\n","!wget https://www.dropbox.com/s/qdcu37dg4w2adwy/handout.tar.gz?dl=0 -O data/handout.tar.gz -q"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"uuadxS4onR0u","executionInfo":{"status":"ok","timestamp":1618209431463,"user_tz":-180,"elapsed":305279,"user":{"displayName":"Лена","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiMK7DQoNpvJJ9b_baTOhOl-0KsCYViUs3aLi0N=s64","userId":"14206305893323667740"}},"outputId":"89edb210-a799-4e5e-e0c3-387f5c9e2464"},"source":["# распакуем архив\n","!tar -xvf data/handout.tar.gz"],"execution_count":null,"outputs":[{"output_type":"stream","text":["data/\n","data/captions_tokenized.json\n","data/image_codes.npy\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"qAzMRdieH8yS"},"source":["Установим необходимые библиотеки и импортируем их."]},{"cell_type":"code","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","id":"K3l4InIXmp_H","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1618209439420,"user_tz":-180,"elapsed":312856,"user":{"displayName":"Лена","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiMK7DQoNpvJJ9b_baTOhOl-0KsCYViUs3aLi0N=s64","userId":"14206305893323667740"}},"outputId":"d3800721-0486-402b-9074-c9727bd3a824"},"source":["!pip install transformers==2.11.0 -U tokenizers==0.7.0 -q"],"execution_count":null,"outputs":[{"output_type":"stream","text":["\u001b[K     |████████████████████████████████| 675kB 4.2MB/s \n","\u001b[K     |████████████████████████████████| 5.6MB 22.0MB/s \n","\u001b[K     |████████████████████████████████| 870kB 36.7MB/s \n","\u001b[K     |████████████████████████████████| 1.2MB 39.5MB/s \n","\u001b[?25h  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Sw0pqPfwmp_N"},"source":["!pip install scipy nltk -q"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","id":"ykdGj5CEmp_R"},"source":["import random\n","import numpy as np\n","import json\n","\n","from matplotlib import pyplot as plt\n","from PIL import Image\n","\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","from sklearn.model_selection import train_test_split\n","\n","from nltk.translate.bleu_score import corpus_bleu\n","from nltk.translate.bleu_score import SmoothingFunction\n","\n","from tqdm.notebook import tqdm\n","from tokenizers import BertWordPieceTokenizer\n","from torch.utils.data import DataLoader, Dataset"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"U1gVDcj1mp_W"},"source":["# загружаем датасет\n","img_codes = np.load('data/image_codes.npy')\n","captions = json.load(open('data/captions_tokenized.json'))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"M_10pvBLIDts"},"source":["Посмотрим на данные."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9RdroLNpmp_a","executionInfo":{"status":"ok","timestamp":1618209449477,"user_tz":-180,"elapsed":321755,"user":{"displayName":"Лена","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiMK7DQoNpvJJ9b_baTOhOl-0KsCYViUs3aLi0N=s64","userId":"14206305893323667740"}},"outputId":"bd94262a-8ce0-40bc-abd4-dbb0ad97a84f"},"source":["print('Количество предложений: ', len(captions), '\\nКоличество изображений: ', len(img_codes))\n","print('Тип данных предложений: ', type(captions), '\\nТип данных изображений: ', type(img_codes))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Количество предложений:  118287 \n","Количество изображений:  118287\n","Тип данных предложений:  <class 'list'> \n","Тип данных изображений:  <class 'numpy.ndarray'>\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1ChsroaMHhrL","executionInfo":{"status":"ok","timestamp":1618209449478,"user_tz":-180,"elapsed":321576,"user":{"displayName":"Лена","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiMK7DQoNpvJJ9b_baTOhOl-0KsCYViUs3aLi0N=s64","userId":"14206305893323667740"}},"outputId":"6a9542d1-cc44-4a63-9f40-8b684e74a529"},"source":["# посмотрим размерность данных\n","img_codes.shape"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(118287, 2048)"]},"metadata":{"tags":[]},"execution_count":14}]},{"cell_type":"markdown","metadata":{"id":"lzpY6a8yIKUZ"},"source":["Для каждого изображения есть 5 описаний!"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"somyQwiMI2vA","executionInfo":{"status":"ok","timestamp":1618209449478,"user_tz":-180,"elapsed":321084,"user":{"displayName":"Лена","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiMK7DQoNpvJJ9b_baTOhOl-0KsCYViUs3aLi0N=s64","userId":"14206305893323667740"}},"outputId":"d15510b4-2691-403a-af6c-f75967154e2a"},"source":["captions[0]"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['people shopping in an open market for vegetables .',\n"," 'an open market full of people and piles of vegetables .',\n"," 'people are shopping at an open air produce market .',\n"," 'large piles of carrots and potatoes at a crowded outdoor market .',\n"," 'people shop for vegetables like carrots and potatoes at an open air market .']"]},"metadata":{"tags":[]},"execution_count":15}]},{"cell_type":"markdown","metadata":{"id":"nZE4sZaAIQXp"},"source":["Чтобы работать с текстовыми данными, нам нужно их конвертировать в числа. Для этого нам необходим токенайзер, который создаст словарь и каждому слову в словаре поставит в соответствие число."]},{"cell_type":"markdown","metadata":{"id":"5_BUxurLJgCk"},"source":["Будем тренировать с нуля свой токенайзер. Для этого нам необходимо создать файл txt, в котором будут записаны через запятую все наши примеры с описаниями."]},{"cell_type":"code","metadata":{"id":"LReAjgommp_f"},"source":["def train_tokenizer(captions):\n","    print('Create training file...')\n","    # создадим список со всеми описаниями:\n","    # [[str, str...], [str, str...], [str, str...]] >>> [str, str, ..., str]\n","    train_tokenizer = [sample for samples in captions for sample in samples]\n","    # запишем список в файл txt, который потом подадим на обучению токенайзеру \n","    with open('train_tokenizer.txt', 'a') as f:\n","        for sample in train_tokenizer:\n","            f.write(sample)\n","    # инициализируем токенайзер\n","    # https://huggingface.co/docs/tokenizers/python/latest/api/reference.html#tokenizer\n","    bwpt = BertWordPieceTokenizer(vocab_file=None,                           # использовать готовый словарь. None: сгенерирует словарь \n","                              unk_token='[UNK]',\n","                              sep_token='[SEP]',\n","                              cls_token='[CLS]',\n","                              clean_text=True,                               # очистить текст от знаков препинания и других ненужных символов\n","                              handle_chinese_chars=True,                     # использовать китайские иероглифы\n","                              strip_accents=True,                            # удалить знаки препинания\n","                              lowercase=True,                                # привести к нижнему регистру\n","                              wordpieces_prefix='##')                        # префикс для частей слова\n","                                                                             # например, слово sleeping будет разбито: sleep, ##ing\n","    # тренировка токенайзера\n","    print('Tokenizer training...')\n","    bwpt.train( files=['train_tokenizer.txt'],                               # укажем файл с тренировочным множеством\n","            vocab_size=30000,                                                # зададим размер словаря\n","            min_frequency=5,                                                 # зададим с какой частотой оставим слова\n","            limit_alphabet=1000,                                             # максимальное кол-во символов, которое нужно сохранить в алфавите\n","            special_tokens=['[PAD]', '[UNK]', '[CLS]', '[MASK]', '[SEP]'] )  # добавим специальные токены\n","    \n","    # сохраним натренированный токенайзер и словарь\n","    bwpt.save('.', 'captions')\n","    \n","    # инициализируем токенайзер используя сгенерированный словарь\n","    tokenizer = BertWordPieceTokenizer('captions-vocab.txt')\n","    tokenizer.enable_truncation(max_length=16)\n","    print('Tokenizer is ready to use...')\n","    return tokenizer"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ZbfWV24DTBIv"},"source":["Натренируем наш токенайзер и инициализируем его"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9abdH1BXmp_j","executionInfo":{"status":"ok","timestamp":1618209494502,"user_tz":-180,"elapsed":365162,"user":{"displayName":"Лена","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiMK7DQoNpvJJ9b_baTOhOl-0KsCYViUs3aLi0N=s64","userId":"14206305893323667740"}},"outputId":"b9f1b7a4-5af1-4541-af84-0fdde10f247b"},"source":["tokenizer = train_tokenizer(captions)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Create training file...\n","Tokenizer training...\n","Tokenizer is ready to use...\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"BNUyedqzTFFZ"},"source":["Создадим датасет"]},{"cell_type":"code","metadata":{"id":"P_mzQor5mp_x"},"source":["class ExtDataset(Dataset):\n","    def __init__(self, images, captions, tokenizer):\n","        self.images = images\n","        self.captions = captions\n","        self.tokenizer = tokenizer\n","\n","    # возвращает кол-во примеров в датасете\n","    def __len__(self):\n","        return len(self.images)\n","    \n","    # по индексу возвращает словарь, в котором наше описани и изображение в векторизированном виде\n","    def __getitem__(self, idx):\n","        # по индексу возьмем вектор изображения\n","        image = self.images[idx].tolist()\n","        # по индексу возьмем описание, будем брать первое описание из 5 доступных\n","        sample = self.captions[idx][0] \n","        # токенизируем описание ['the dog is sleeping'] >>> ['the', 'dog', 'is', 'sleep', 'ing']\n","        # вызвав метод ids: ['the', 'dog', 'is', 'sleep', 'ing'] >>> [100, 8655, 12, 96, 888, 64, 101]\n","        # 100: обозначает токен начала предложения [CLS]\n","        # 101: обозначает токен конца предложения [SEP]\n","        caption = self.tokenizer.encode(sample).ids\n","\n","        return {'inputs': image, 'outputs': caption}"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"I4IYFQODUwe3"},"source":["У нас все предложения имеют разную длину, нам нужно в нейросеть подавать батчи одинаковой размерности, для этого нам нужно знать максимальную длину предложения, чтобы все вектора добить паддингом до максимальной длины.\n","\n","Т.к. мы используем архитектуру RNN, по своемпостроению она медленная, и если мы будем каждый раз обрабатывать вектор максимальной длины, то обучение будет слишком долгим.\n","\n","Чтобы решить эту проблему, мы посмотрим распределение длины описаний и возьмем минимально возможную длину, которая охватывает максимальное количество примеров. Описания большей длины будут обрезаны."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XqrhF1GYmp_1","executionInfo":{"status":"ok","timestamp":1618209495271,"user_tz":-180,"elapsed":365141,"user":{"displayName":"Лена","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiMK7DQoNpvJJ9b_baTOhOl-0KsCYViUs3aLi0N=s64","userId":"14206305893323667740"}},"outputId":"e748f668-b27a-4ab9-eb4f-5c9bfc040619"},"source":["max_len = max(len(sample.split()) for samples in captions for sample in samples)\n","max_len"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["57"]},"metadata":{"tags":[]},"execution_count":19}]},{"cell_type":"code","metadata":{"id":"YEXKGMGFmp_5"},"source":["# посчитаем количество слов в каждом описании\n","distribution = [len(sample.split()) for samples in captions for sample in samples]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"1vMYcXSZNA3W"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":500},"id":"HCiW2y3wmp_-","executionInfo":{"status":"ok","timestamp":1618209495855,"user_tz":-180,"elapsed":365116,"user":{"displayName":"Лена","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiMK7DQoNpvJJ9b_baTOhOl-0KsCYViUs3aLi0N=s64","userId":"14206305893323667740"}},"outputId":"d5923ac1-b1aa-418b-a02f-f9a7e0bdd164"},"source":["import matplotlib.pyplot as plt\n","%matplotlib inline\n","\n","plt.figure(figsize=(12,8))\n","plt.hist(distribution, bins=30)  # `density=False` would make counts\n","plt.xticks(range(0, 40, 2))\n","plt.ylabel('Frequency')\n","plt.xlabel('Data');"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 864x576 with 1 Axes>"]},"metadata":{"tags":[],"needs_background":"light"}}]},{"cell_type":"markdown","metadata":{"id":"xl97Cn4vWVB1"},"source":["Из гистограммы видно, что если мы возьмем максимальную длину предложения 16, то охватим достаточно много примеров."]},{"cell_type":"markdown","metadata":{"id":"QsKy87dYW0uu"},"source":["Создадим функцию, в которой настроим, как нам необходимо разбить датасет на батчи. В нашем случае мы возьмем максимальную длину 16, не забудем про спец. токены начала и конца предложения, итого максимальная длина будет 18."]},{"cell_type":"markdown","metadata":{"id":"iDFJeufbZkJ7"},"source":["Обратим внимание, что image_dim это вектор с извлеченными фичами из CNN модели! Вектор, который обычно далее направляется в полносвязный слой для классификации."]},{"cell_type":"markdown","metadata":{"id":"Pa3yeP8nhn_D"},"source":["Чтобы сгенерировать батчи, мы создадим матрицы с нулевыми значениями, т.к. ноль у нас по умолчанию является индексом паддинга. Паддинг - это значение, которым \"добиваем\" вектор до нужной длины.\n","\n","Пример добавления паддинга было >>> стало\n","\n","\n","[100, 83, 5621, 8753, 101] >>> [100, 83, 5621, 8753, 101, 0, 0, 0, ..., max_len]"]},{"cell_type":"code","metadata":{"id":"P3Us07JSmqAC"},"source":["def collate_fn(dataset, max_len=16, image_dim=2048):\n","  # укажем максимальную длину, не забудем про спец. токены.\n","  max_len = max_len + 2 # bos & eos\n","\n","  # инициализируем две матрицы состоящих из нулей, в которые будем добавлять вектора описаний и изображений\n","  # new_inputs нулевая матрица для изображений, размерность [кол-во примеров x размерность вектора изображения] \n","  new_inputs = torch.zeros((len(dataset), image_dim), dtype=torch.float)\n","  # new_outputs нулевая матрица для описаний, размерность [кол-во примеров x макс. длина вектора, кот-ю мы определили]\n","  new_outputs = torch.zeros((len(dataset), max_len), dtype=torch.long)\n","  # пробежимся по всему датасету\n","  # sample: {'inputs': image, 'outputs': caption} как определили в class ExtDataset\n","  for i, sample in enumerate(dataset):\n","    # берем i-ю строку и добавляем в неё вектор изображения [0, 0, ..., 0] >>> [8.65001, 1.0012, ..., 0] \n","    new_inputs[i, :] += np.array(sample['inputs'])\n","    # берем i-ю строку и добавляем в неё вектор описания [0, 0, ..., 0] >>> [100, 820, ..., 0] \n","    new_outputs[i, :len(sample['outputs'])] += np.array(sample['outputs'])\n","  # заполненные матрицы возвращаем\n","  return {'input_ids': new_inputs, 'outputs': new_outputs}"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"togHqD0bfrhW"},"source":["Создадим наш энкодер, который будет брать вектор изображения и кодировать его в размерность скрытого состояния нашей RNN модели. Далее выход энкодера будем подавать на вход RNN системе в качестве скрытого состояния."]},{"cell_type":"code","metadata":{"id":"z6wgCQICmqAG"},"source":["class Encoder(nn.Module):\n","    # hid_dim: вектор скрытого состояния\n","    # n_layers: кол-во слоёв в RNN архитектуре, мы планируем использовать LSTM с 2-мя слоями\n","    # для этого нам нужно подготовить правильную размерность для скрытых состояний [n_layres, bs, hid_dim]\n","    # cnn_feature_dim: размерность извлеченного CNN моделью, вектора с фичами из изображения\n","    def __init__(self, hid_dim=512, n_layers=2, cnn_feature_dim=2048):\n","        super(Encoder, self).__init__()\n","        \n","        self.n_layers = n_layers\n","        # инициализируем линейные слои\n","        self.cnn2h0 = nn.Linear(cnn_feature_dim, hid_dim)\n","        self.cnn2c0 = nn.Linear(cnn_feature_dim, hid_dim)\n","  \n","    def forward(self, image_vectors):       \n","        # input\n","        # image_vectors: [bs, cnn_feature_dim]\n","        # output\n","        # cnn2h0: [bs, hid_dim]\n","        # unsqueeze(0).repeat: [n_layers, bs, hid_dim]\n","        initial_hid = self.cnn2h0(image_vectors).unsqueeze(0).repeat(self.n_layers, 1, 1)\n","        initial_cell = self.cnn2c0(image_vectors).unsqueeze(0).repeat(self.n_layers, 1, 1)\n","        return initial_hid, initial_cell"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"YFtzp7MZjBD4"},"source":["Создадим модель, которая на вход будет принимать вектор с фичами извлеченными с помощью CNN из изображения, а выдавать логиты, распределение на слова в словаре."]},{"cell_type":"code","metadata":{"id":"RY0vjRFamqAK"},"source":["class CaptionNet(nn.Module):\n","    # emb_dim: размерность вектора слова\n","    # hid_dim: размерность скрытого состояния RNN\n","    # n_layers: кол-во слоёв в RNN\n","    # cnn_feature_dim: размерность извлеченного CNN моделью, вектора с фичами из изображения\n","    # vocab_size: размер словаря\n","    def __init__(self, emb_dim=256, hid_dim=512, n_layers=2, cnn_feature_dim=2048, dropout=0.3, vocab_size=30000):\n","        super(CaptionNet, self).__init__()\n","\n","        self.vocab_size = vocab_size\n","        # создадим матрицу хранящую эмбеддинги (вектора) слов\n","        self.embedding = nn.Embedding(vocab_size, emb_dim)\n","            \n","        # инициализируем lstm модель\n","        self.lstm = nn.LSTM(emb_dim, hid_dim, n_layers)\n","            \n","        # инициализируем линейный слой для получения логитов\n","        # hid_dim * 2: расширение с последующим сужением вектора признаков используется в BERT, даёт лучший результат \n","        self.fc1 = nn.Linear(hid_dim, hid_dim * 2)\n","        self.fc2 = nn.Linear(hid_dim * 2 , vocab_size)\n","        self.dropout = nn.Dropout(dropout)\n","\n","\n","    def forward(self, captions_ix, initial_hid, initial_cell):\n","        # [bs] >>> [1, bs]\n","        captions_ix = captions_ix.unsqueeze(0)\n","        # [1, bs, emb_dim]\n","        captions_emb = self.embedding(captions_ix)\n","        \n","        captions_emb = self.dropout(captions_emb)\n","        \n","        # outputs: [1, bs, hid_dim]\n","        # hidden and cell: [n_layers, bs, hid_dim]\n","        outputs, (hidden, cell) = self.lstm(captions_emb, (initial_hid, initial_cell)) # shape: [batch, caption_length, lstm_units]\n","        \n","        # logits: [bs, vocab_size]        \n","        logits = self.fc2(F.relu( self.fc1(outputs.squeeze()) ))\n","        \n","        return logits, hidden, cell"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"P9KSEW0qi_mX"},"source":["Соберем нашу модель из энкодера и декодера. Будем использовать тренировку методом Teacher forcing. В каждый момент времени t, когда модель сгенерировала токен, будем определять, на следующий шаг t+1 отдавать модели сгенерированный ею токен или правильный (как в описании)."]},{"cell_type":"code","metadata":{"id":"V8UFiCefmqAQ"},"source":["class ImgCap(nn.Module):\n","    def __init__(self, encoder, decoder, device, max_len):\n","        super(ImgCap, self).__init__()\n","\n","        self.encoder = encoder\n","        self.decoder = decoder\n","        self.device = device\n","        self.max_len = max_len\n","        self.vocab_size = self.decoder.vocab_size\n","\n","    def forward(self, image_vectors, captions, teacher_forcing_ratio = 0.5):\n","        # input\n","        # image_vectors: [bs, image_vec]\n","        # captions: [len_seq, bs]\n","\n","        # запишем размер батча и длину последовательности\n","        batch_size = image_vectors.shape[0]\n","        seq_len = captions.shape[0]\n","\n","        # инициализируем тензор с нулевыми матрицами, куда будем записывать сгенерированные описания\n","        # [seq_len, bs, vocab_size]\n","        outputs = torch.zeros(seq_len, batch_size, self.vocab_size).to(self.device)\n","        # hidden and cell: [n_layers, bs, hid_dim]\n","        hidden, cell = self.encoder(image_vectors)\n","        # возьмем первые токены [CLS] из всех описаний в батче, чтобы отдать для генерации описания нашей RNN\n","        # input: [bs]\n","        input = captions[0]\n","        # будем генерировать токены исходя из длины последовательности (макс. длина, которую мы определили выше в def collate_fn)\n","        for t in range(1, seq_len):\n","            # output: [bs, vocab_size]\n","            # hidden and cell: [n_layers, bs, hid_dim]\n","            output, hidden, cell = self.decoder(input, hidden, cell)\n","            # outputs[t]: [bs, vocab_size]\n","            outputs[t] = output\n","            # random.random() - random number from 0 to 1\n","            teacher_force = random.random() < teacher_forcing_ratio\n","            # выберем самое вероятное слово из распределения для каждого описания в батче\n","            top1 = output.max(1)[1]\n","            # определяем, будем использовать сгененированные моделью токены или отдадим им правильные \n","            input = (captions[t] if teacher_force else top1)\n","        return outputs\n","\n","    def generate_one_example(self, imagepath, inception):\n","        \n","        test_image = face_recognition.load_image_file(imagePath)\n","  \n","        # Find all the faces in the test image using the default HOG-based model\n","        face_locations = face_recognition.face_locations(test_image)\n","      \n","        # Predict all the faces in the test image using the trained classifier\n","        names = []\n","        for i in range(len(face_locations)):\n","      \n","          test_image_enc = face_recognition.face_encodings(test_image)[i]\n","          names.append(clf.predict([test_image_enc]))\n","        \n","\n","           \n","        # image: [299, 299, 3] -> [width, height, channel]\n","        # inception: модель CNN для извлечения фич из изображения\n","\n","        # [width, height, channel] >> [channel, width, height]\n","        image = Image.open(imagePath)\n","        image = np.array(image.resize((299, 299))).astype('float32') / 255.\n","        image = torch.tensor(image.transpose([2, 0, 1]), dtype=torch.float32)\n","\n","        # извлечем фичи из изображения, нам понадобится вектор vectors_neck\n","        vectors_8x8, vectors_neck, logits = inception(image[None])\n","\n","        outputs = []\n","\n","        # поместим данные на gpu\n","        image_vectors = vectors_neck.to(self.device)\n","\n","        #получим скрытое состояние из RNN\n","        hidden, cell = self.encoder(image_vectors)\n","\n","        # в токенайзере возьмем id токена означающего начало описания \n","        input = torch.tensor([tokenizer.token_to_id('[CLS]')]).to(device)\n","\n","        # здесь вероятно нужно self.max_len, нужно уточнить\n","        for t in range(1, max_len):\n","            # output: [bs=1, vocab_size]\n","            output, hidden, cell = decoder(input, hidden, cell)\n","            # из распределения слов в словаре возьмем токен с самым высоким значением\n","            top1 = output.max(0)[1]\n","            outputs.append(top1)\n","            # добавим размерность, т.к. decoder принимает данные с размерностью [1, bs]\n","            input = (top1.unsqueeze(0))\n","\n","        # у токенайзера возьмем id токена, означающего конец описания\n","        EOS_IDX = tokenizer.token_to_id('[SEP]')\n","        # возьмем последовательностей токенов и сгенерируем описание [100, 97893, 347, 735, 101] >>> [the dog is sleeping]\n","        k = 0\n","        for t in outputs:\n","            if t.item() != EOS_IDX:\n","              word = tokenizer.id_to_token(t.item())\n","              if word in ['person', 'man', 'woman']:\n","                print(names[k], end=' ')\n","                k+=1\n","              else:\n","                print(word, end=' ')\n","            else:\n","                break"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"aS38QGhlmqAU"},"source":["#@title Set parameters { vertical-output: true, display-mode: \"form\" }\n","\n","lr = 1e-3 #@param\n","emb_dim = 128 #@param\n","hid_dim = 256 #@param\n","n_layers = 2 #@param\n","dropout = 0.3 #@param\n","batch_size = 300 #@param\n","num_epochs = 20 #@param\n","\n","clip = 5\n","max_len = 18\n","vocab_size = 30000\n","cnn_feature_dim = 2048\n","\n","device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n","PAD_IDX = tokenizer.token_to_id('[PAD]')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"V-BSg9FzAtcG"},"source":["Инициализируем нашу модель и поместим на gpu"]},{"cell_type":"code","metadata":{"id":"Nxy1qi2umqAY"},"source":["encoder = Encoder(hid_dim, n_layers, cnn_feature_dim).to(device)\n","decoder = CaptionNet(emb_dim, hid_dim, n_layers, cnn_feature_dim, dropout, vocab_size).to(device)\n","model = ImgCap(encoder, decoder, device, max_len).to(device)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"CITBPgDpAzNL"},"source":["Инициализируем оптимайзер и лосс функцию. Важно! Лосс функции нужно указать id паддинга, чтобы он не учитывался при вычислении функции потерь."]},{"cell_type":"code","metadata":{"id":"nXurds3EmqAc"},"source":["PAD_IDX = tokenizer.token_to_id('[PAD]')\n","optimizer = optim.Adam(model.parameters(), lr=1e-3)\n","criterion = nn.CrossEntropyLoss(ignore_index=PAD_IDX)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"spwmJRGMBG76"},"source":["Напишем функцию, которая вычисляет метрику BLUE, будем в ходе обучения наблюдать за данной метрикой."]},{"cell_type":"code","metadata":{"id":"vyxzpa1YmqAf"},"source":["def get_blue(logits, captions):\n","    \n","    predict = torch.argmax(logits, -1)\n","    \n","    sentences = []\n","    targets = []\n","\n","    for i in range(predict.shape[-1]):\n","        sentence_ids = predict[:, i].tolist()\n","        sentence = tokenizer.decode(sentence_ids)\n","        target_ids = captions[:, i].tolist()\n","        target = tokenizer.decode(target_ids)\n","        sentences.append([sentence])\n","        targets.append(target)\n","    \n","    return corpus_bleu(sentences, targets, smoothing_function=SmoothingFunction().method1)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"RxujXPFrBgi0"},"source":["Напишем тренировочный цикл."]},{"cell_type":"code","metadata":{"id":"lTTCm2G3mqAj"},"source":["def train(model, dataloader, optimizer, criterion, clip):\n","    \n","    model.train()\n","    epoch_loss = 0.\n","    blue = 0.\n","    \n","    for batch in dataloader:\n","        \n","        # [bs, image_vec]\n","        image_vectors = batch['input_ids'].to(device)        \n","        # [bs, len_seq] \n","        captions = batch['outputs'].to(device)    \n","        # [len_seq, bs]\n","        captions = torch.transpose(captions, 1, 0)      \n","\n","        optimizer.zero_grad()                      \n","        \n","        # [len_seq, bs, vocab_size]\n","        logits = model(image_vectors, captions)\n","\n","        blue += get_blue(logits, captions)\n","\n","        # сделаем необходимые решейпы для подачи данных в функцию потерь, не будем учитывать первые токены описаний\n","        # contiguous: вкратце, для корректных вычислений после изменений размерности  https://stackoverflow.com/questions/48915810/pytorch-contiguous\n","        # [(len_seq - 1) * bs, vocab_size]\n","        logits = logits[1:].contiguous().view(-1, logits.shape[-1]) \n","        # [(len_seq - 1) * bs]\n","        captions = captions[1:].contiguous().view(-1)\n","\n","        loss = criterion(logits, captions)\n","        \n","        loss.backward()\n","        \n","        # в RNN узкое место, это взрывающиеся градиенты, чтобы измежать числовой нестабильности, мы клипаем градиенты, если они большие\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n","        \n","        optimizer.step()\n","        \n","        epoch_loss += loss.item()\n","        \n","    train_loss = round( (epoch_loss / len(dataloader)), 3)\n","    \n","    blue_mean = round((blue / len(dataloader)), 3) * 100\n","        \n","    return train_loss, blue_mean"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"weH0CcbKmqAm"},"source":["def evaluate(model, dataloader, criterion):\n","    \n","    model.eval()\n","    epoch_loss = 0\n","    blue = 0.\n","    \n","    with torch.no_grad():\n","    \n","        for batch in dataloader:\n","\n","            image_vectors = batch['input_ids'].to(device)\n","            captions = batch['outputs'].to(device)\n","            captions = torch.transpose(captions, 1, 0)\n","\n","            logits = model(image_vectors, captions)\n","            \n","            blue += get_blue(logits, captions)\n","\n","            logits = logits[1:].contiguous().view(-1, logits.shape[-1])\n","            \n","            captions = captions[1:].contiguous().view(-1)\n","\n","            loss = criterion(logits, captions)\n","            \n","            epoch_loss += loss.item()\n","            \n","        valid_loss = round((epoch_loss / len(dataloader)), 3)\n","        \n","        blue_mean = round((blue / len(dataloader)), 3) * 100\n","        \n","    return valid_loss, blue_mean"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"vlc6_jziD4h3"},"source":["Подготовим наши данные."]},{"cell_type":"code","metadata":{"id":"H6xReyYNmqAp"},"source":["# разделим выборку на тестовую и валидационную\n","random.seed(42)\n","np.random.seed(42)\n","captions = np.array(captions)\n","train_img_codes, val_img_codes, train_captions, val_captions = train_test_split(img_codes, captions,\n","                                                                                test_size=0.1,\n","                                                                                random_state=42)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-gLl2tYUmqAs"},"source":["train_dl = DataLoader(ExtDataset(train_img_codes, train_captions, tokenizer), batch_size=batch_size, collate_fn=collate_fn)\n","val_dl = DataLoader(ExtDataset(val_img_codes, val_captions, tokenizer), batch_size=batch_size, collate_fn=collate_fn)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"mlcFv2o3EG5g"},"source":["Запустим тренировочный цикл."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":956,"referenced_widgets":["d76090fa28ec46c18d6619de89b21007","c6a8a8dd2d0a452bb45726317ce84d91","7b86dc97da4a4461bf766ed4f833292f","b7c7251df4dc4f2ebb834090c4690347","059c390c7a024996b56b434e2dd25871","ee11fb19751a4fd89cf169792232af82","29936adb1ef54ad5a352cf8ee5626568","aa579e0c69034b4f856e2c99a7f2ad8e"]},"id":"sH6zHWAumqAx","executionInfo":{"status":"error","timestamp":1618215531398,"user_tz":-180,"elapsed":6394712,"user":{"displayName":"Лена","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiMK7DQoNpvJJ9b_baTOhOl-0KsCYViUs3aLi0N=s64","userId":"14206305893323667740"}},"outputId":"4c00fec2-5495-46eb-a20d-150a13802b48"},"source":["best_blue = 20.\n","for epoch in tqdm(range(num_epochs)):\n","  \n","    train_loss, train_blue_mean = train(model, train_dl, optimizer, criterion, clip)\n","    valid_loss, val_blue_mean = evaluate(model, val_dl, criterion)\n","\n","    print('Epoch: {} \\n Train Loss {}  Val loss {} \\n Train Blue {} Val Blue {}:'.format(epoch + 1, train_loss, valid_loss, train_blue_mean, val_blue_mean))\n","\n","    # сохраним модель с лучшей метрикой BLUE\n","    if best_blue < val_blue_mean:\n","        best_blue = val_blue_mean\n","        torch.save(model.state_dict(), 'imgcap.pth')"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"d76090fa28ec46c18d6619de89b21007","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, max=20.0), HTML(value='')))"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["Epoch: 1 \n"," Train Loss 4.765  Val loss 4.238 \n"," Train Blue 9.8 Val Blue 16.900000000000002:\n","Epoch: 2 \n"," Train Loss 3.977  Val loss 3.794 \n"," Train Blue 22.1 Val Blue 26.5:\n","Epoch: 3 \n"," Train Loss 3.684  Val loss 3.615 \n"," Train Blue 28.7 Val Blue 29.9:\n","Epoch: 4 \n"," Train Loss 3.496  Val loss 3.551 \n"," Train Blue 31.8 Val Blue 33.0:\n","Epoch: 5 \n"," Train Loss 3.414  Val loss 3.525 \n"," Train Blue 33.300000000000004 Val Blue 32.300000000000004:\n","Epoch: 6 \n"," Train Loss 3.329  Val loss 3.459 \n"," Train Blue 34.300000000000004 Val Blue 34.699999999999996:\n","Epoch: 7 \n"," Train Loss 3.271  Val loss 3.459 \n"," Train Blue 35.0 Val Blue 34.4:\n","Epoch: 8 \n"," Train Loss 3.237  Val loss 3.374 \n"," Train Blue 35.5 Val Blue 35.4:\n","Epoch: 9 \n"," Train Loss 3.195  Val loss 3.406 \n"," Train Blue 36.1 Val Blue 35.4:\n","Epoch: 10 \n"," Train Loss 3.148  Val loss 3.339 \n"," Train Blue 36.5 Val Blue 36.6:\n","Epoch: 11 \n"," Train Loss 3.114  Val loss 3.457 \n"," Train Blue 36.9 Val Blue 36.1:\n"],"name":"stdout"},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-34-cab12e56a956>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mtrain_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_blue_mean\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclip\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mvalid_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_blue_mean\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_dl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-30-f9be179caea1>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, dataloader, optimizer, criterion, clip)\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0;31m# в RNN узкое место, это взрывающиеся градиенты, чтобы измежать числовой нестабильности, мы клипаем градиенты, если они большие\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip_grad_norm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclip\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py\u001b[0m in \u001b[0;36mclip_grad_norm_\u001b[0;34m(parameters, max_norm, norm_type)\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0mtotal_norm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0mclip_coef\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax_norm\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtotal_norm\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1e-6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0mclip_coef\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m             \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclip_coef\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"markdown","metadata":{"id":"v7RlbNMmEQE3"},"source":["Загрузим лучшую модель."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Cal5Ud8smqA2","executionInfo":{"status":"ok","timestamp":1618215535307,"user_tz":-180,"elapsed":1516,"user":{"displayName":"Лена","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiMK7DQoNpvJJ9b_baTOhOl-0KsCYViUs3aLi0N=s64","userId":"14206305893323667740"}},"outputId":"1b4c5bd4-a11d-4d3c-e1b4-b4ca8625dc2e"},"source":["model.load_state_dict(torch.load('imgcap.pth'))"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<All keys matched successfully>"]},"metadata":{"tags":[]},"execution_count":35}]},{"cell_type":"markdown","metadata":{"id":"lYPYp3CGmqA5"},"source":["Скачаем обученную CNN модель inception и инициализируем её."]},{"cell_type":"code","metadata":{"id":"b8LVu9D3mqA5"},"source":["from torch.autograd import Variable\n","from torchvision.models.inception import Inception3\n","from warnings import warn\n","\n","class BeheadedInception3(Inception3):\n","    \"\"\" Like torchvision.models.inception.Inception3 but the head goes separately \"\"\"\n","    \n","    def forward(self, x):\n","        if self.transform_input:\n","            x = x.clone()\n","            x[:, 0] = x[:, 0] * (0.229 / 0.5) + (0.485 - 0.5) / 0.5\n","            x[:, 1] = x[:, 1] * (0.224 / 0.5) + (0.456 - 0.5) / 0.5\n","            x[:, 2] = x[:, 2] * (0.225 / 0.5) + (0.406 - 0.5) / 0.5\n","        else: warn(\"Input isn't transformed\")\n","        x = self.Conv2d_1a_3x3(x)\n","        x = self.Conv2d_2a_3x3(x)\n","        x = self.Conv2d_2b_3x3(x)\n","        x = F.max_pool2d(x, kernel_size=3, stride=2)\n","        x = self.Conv2d_3b_1x1(x)\n","        x = self.Conv2d_4a_3x3(x)\n","        x = F.max_pool2d(x, kernel_size=3, stride=2)\n","        x = self.Mixed_5b(x)\n","        x = self.Mixed_5c(x)\n","        x = self.Mixed_5d(x)\n","        x = self.Mixed_6a(x)\n","        x = self.Mixed_6b(x)\n","        x = self.Mixed_6c(x)\n","        x = self.Mixed_6d(x)\n","        x = self.Mixed_6e(x)\n","        x = self.Mixed_7a(x)\n","        x = self.Mixed_7b(x)\n","        x_for_attn = x = self.Mixed_7c(x)\n","        # 8 x 8 x 2048\n","        x = F.avg_pool2d(x, kernel_size=8)\n","        # 1 x 1 x 2048\n","        x_for_capt = x = x.view(x.size(0), -1)\n","        # 2048\n","        x = self.fc(x)\n","        # 1000 (num_classes)\n","        return x_for_attn, x_for_capt, x\n","    \n","from torch.utils.model_zoo import load_url\n","def beheaded_inception_v3(transform_input=True):\n","    model= BeheadedInception3(transform_input=transform_input)\n","    inception_url = 'https://download.pytorch.org/models/inception_v3_google-1a9a5a14.pth'\n","    model.load_state_dict(load_url(inception_url))\n","    return model"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":136,"referenced_widgets":["783052fafaa843ea8a850a3901437b29","c75e2bfd3a124df6b5e889f0fc0f7046","540e72dc25944da8af8c2d582edddf84","7441ecd69209422ab539aebbdb9262d0","4159269a79b5488481a25a4666a25982","81629f49d9df4a6092d1154e61bf9793","f2e46df31d324cc9b70bd5c9116f9b91","85e4e6d6053342fa91f0dc6c922fbcea"]},"id":"MRwSYLlXmqA9","executionInfo":{"status":"ok","timestamp":1618215823128,"user_tz":-180,"elapsed":288758,"user":{"displayName":"Лена","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiMK7DQoNpvJJ9b_baTOhOl-0KsCYViUs3aLi0N=s64","userId":"14206305893323667740"}},"outputId":"8d0ef006-a40d-451c-881c-4b5fb7ed79f0"},"source":["inception = beheaded_inception_v3().train(False)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/torchvision/models/inception.py:82: FutureWarning: The default weight initialization of inception_v3 will be changed in future releases of torchvision. If you wish to keep the old behavior (which leads to long initialization times due to scipy/scipy#11299), please set init_weights=True.\n","  ' due to scipy/scipy#11299), please set init_weights=True.', FutureWarning)\n","Downloading: \"https://download.pytorch.org/models/inception_v3_google-1a9a5a14.pth\" to /root/.cache/torch/hub/checkpoints/inception_v3_google-1a9a5a14.pth\n"],"name":"stderr"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"783052fafaa843ea8a850a3901437b29","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, max=108857766.0), HTML(value='')))"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"R07W06qRmqBA"},"source":["Сгенерируем описание изображений."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"output_embedded_package_id":"1_Hv8cC8FKqWVyhDbK06DPnmTqNEdtNt1"},"id":"ZzuesAcjqJus","executionInfo":{"status":"ok","timestamp":1618215856791,"user_tz":-180,"elapsed":322059,"user":{"displayName":"Лена","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiMK7DQoNpvJJ9b_baTOhOl-0KsCYViUs3aLi0N=s64","userId":"14206305893323667740"}},"outputId":"8477cbe2-d243-4f89-f739-673ba51f1e47"},"source":["imagePaths = list(paths.list_images('test_captions'))\n","for (i, imagePath) in enumerate(imagePaths):\n","  \n","\n","  model.generate_one_example(imagePath, inception)\n","  image = cv2.imread(imagePath)  \n","  cv2_imshow(image)\n","  cv2.waitKey(0)"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}]},{"cell_type":"code","metadata":{"id":"LwWecfTIqJsL"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_B6AUYijcQcq"},"source":[""],"execution_count":null,"outputs":[]}]}