{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"recognition_faces_nn.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyMI+7VkqP2xBx4C19RCmncQ"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sMWsJf033_ms","executionInfo":{"status":"ok","timestamp":1616501012834,"user_tz":-180,"elapsed":22100,"user":{"displayName":"Лена","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiMK7DQoNpvJJ9b_baTOhOl-0KsCYViUs3aLi0N=s64","userId":"14206305893323667740"}},"outputId":"e72caf32-cb9a-4501-a3e4-47e7b587f264"},"source":["from google.colab import drive\n","drive.mount('/content/drive', force_remount=True)\n","\n","%cd /content/drive/My\\ Drive/\n","%cd broutanlab_course/"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n","/content/drive/My Drive\n","/content/drive/My Drive/broutanlab_course\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"MmR6Vh8e1-oE","executionInfo":{"status":"ok","timestamp":1616501012835,"user_tz":-180,"elapsed":22097,"user":{"displayName":"Лена","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiMK7DQoNpvJJ9b_baTOhOl-0KsCYViUs3aLi0N=s64","userId":"14206305893323667740"}}},"source":[""],"execution_count":1,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"e-hdfqfs1_fy"},"source":["# Step 1: Extract embeddings from face dataset"]},{"cell_type":"code","metadata":{"id":"BQnKteJz4aDP","executionInfo":{"status":"ok","timestamp":1616502872736,"user_tz":-180,"elapsed":872,"user":{"displayName":"Лена","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiMK7DQoNpvJJ9b_baTOhOl-0KsCYViUs3aLi0N=s64","userId":"14206305893323667740"}}},"source":["import numpy as np\n","import cv2\n","from google.colab.patches import cv2_imshow\n","from sklearn.preprocessing import LabelEncoder\n","from sklearn.svm import SVC\n","import pickle\n","import imutils\n","import cv2\n","import os\n","from imutils import paths"],"execution_count":67,"outputs":[]},{"cell_type":"code","metadata":{"id":"nyBBAOXM4jQk","executionInfo":{"status":"ok","timestamp":1616502873120,"user_tz":-180,"elapsed":1047,"user":{"displayName":"Лена","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiMK7DQoNpvJJ9b_baTOhOl-0KsCYViUs3aLi0N=s64","userId":"14206305893323667740"}}},"source":["# load our serialized face detector from disk\n","detector = cv2.dnn.readNetFromCaffe(\"deploy.prototxt.txt\", \"res10_300x300_ssd_iter_140000.caffemodel\")"],"execution_count":68,"outputs":[]},{"cell_type":"code","metadata":{"id":"uq9ZbiTS4oCk","executionInfo":{"status":"ok","timestamp":1616502873120,"user_tz":-180,"elapsed":875,"user":{"displayName":"Лена","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiMK7DQoNpvJJ9b_baTOhOl-0KsCYViUs3aLi0N=s64","userId":"14206305893323667740"}}},"source":["# load our serialized face embedding model from disk\n","embedder = cv2.dnn.readNetFromTorch('openface.nn4.small2.v1.t7')"],"execution_count":69,"outputs":[]},{"cell_type":"code","metadata":{"id":"dEJAmRKxIaqN","executionInfo":{"status":"ok","timestamp":1616502873501,"user_tz":-180,"elapsed":1077,"user":{"displayName":"Лена","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiMK7DQoNpvJJ9b_baTOhOl-0KsCYViUs3aLi0N=s64","userId":"14206305893323667740"}}},"source":["# grab the paths to the input images in our dataset\n","imagePaths = list(paths.list_images('dataset3'))\n","# initialize our lists of extracted facial embeddings and\n","# corresponding people names\n","knownEmbeddings = []\n","knownNames = []\n","# initialize the total number of faces processed\n","total = 0"],"execution_count":70,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mQhMCM3lIyxB","executionInfo":{"status":"ok","timestamp":1616502877772,"user_tz":-180,"elapsed":5161,"user":{"displayName":"Лена","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiMK7DQoNpvJJ9b_baTOhOl-0KsCYViUs3aLi0N=s64","userId":"14206305893323667740"}},"outputId":"13967141-e785-43b3-aa41-62f0dfd82cb7"},"source":["# loop over the image paths\n","for (i, imagePath) in enumerate(imagePaths):\n","\t# extract the person name from the image path\n","\tprint(\"[INFO] processing image {}/{}\".format(i + 1,\tlen(imagePaths)))\n","\tname = imagePath.split(os.path.sep)[-2]\n","\t# load the image, resize it to have a width of 600 pixels (while\n","\t# maintaining the aspect ratio), and then grab the image\n","\t# dimensions\n","\timage = cv2.imread(imagePath)\n","\timage = imutils.resize(image, width=600)\n","\t(h, w) = image.shape[:2]\n","  # construct a blob from the image\n","\timageBlob = cv2.dnn.blobFromImage(cv2.resize(image, (300, 300)), 1.0, (300, 300),\n","\t\t(104.0, 177.0, 123.0), swapRB=False, crop=False)\n","\t# apply OpenCV's deep learning-based face detector to localize\n","\t# faces in the input image\n","\tdetector.setInput(imageBlob)\n","\tdetections = detector.forward()\n","  # ensure at least one face was found\n","\tif len(detections) > 0:\n","\t\t# we're making the assumption that each image has only ONE\n","\t\t# face, so find the bounding box with the largest probability\n","\t\ti = np.argmax(detections[0, 0, :, 2])\n","\t\tconfidence = detections[0, 0, i, 2]\n","\t\t# ensure that the detection with the largest probability also\n","\t\t# means our minimum probability test (thus helping filter out\n","\t\t# weak detections)\n","\t\tif confidence > 0.5:\n","\t\t\t# compute the (x, y)-coordinates of the bounding box for\n","\t\t\t# the face\n","\t\t\tbox = detections[0, 0, i, 3:7] * np.array([w, h, w, h])\n","\t\t\t(startX, startY, endX, endY) = box.astype(\"int\")\n","\t\t\t# extract the face ROI and grab the ROI dimensions\n","\t\t\tface = image[startY:endY, startX:endX]\n","\t\t\t(fH, fW) = face.shape[:2]\n","\t\t\t# ensure the face width and height are sufficiently large\n","\t\t\tif fW < 20 or fH < 20:\n","\t\t\t\tcontinue\n","      # construct a blob for the face ROI, then pass the blob\n","\t\t\t# through our face embedding model to obtain the 128-d\n","\t\t\t# quantification of the face\n","\t\t\tfaceBlob = cv2.dnn.blobFromImage(face, 1.0 / 255,(96, 96), (0, 0, 0), swapRB=True, crop=False)\n","\t\t\tembedder.setInput(faceBlob)\n","\t\t\tvec = embedder.forward()\n","\t\t\t# add the name of the person + corresponding face\n","\t\t\t# embedding to their respective lists\n","\t\t\tknownNames.append(name)\n","\t\t\tknownEmbeddings.append(vec.flatten())\n","\t\t\ttotal += 1"],"execution_count":71,"outputs":[{"output_type":"stream","text":["[INFO] processing image 1/31\n","[INFO] processing image 2/31\n","[INFO] processing image 3/31\n","[INFO] processing image 4/31\n","[INFO] processing image 5/31\n","[INFO] processing image 6/31\n","[INFO] processing image 7/31\n","[INFO] processing image 8/31\n","[INFO] processing image 9/31\n","[INFO] processing image 10/31\n","[INFO] processing image 11/31\n","[INFO] processing image 12/31\n","[INFO] processing image 13/31\n","[INFO] processing image 14/31\n","[INFO] processing image 15/31\n","[INFO] processing image 16/31\n","[INFO] processing image 17/31\n","[INFO] processing image 18/31\n","[INFO] processing image 19/31\n","[INFO] processing image 20/31\n","[INFO] processing image 21/31\n","[INFO] processing image 22/31\n","[INFO] processing image 23/31\n","[INFO] processing image 24/31\n","[INFO] processing image 25/31\n","[INFO] processing image 26/31\n","[INFO] processing image 27/31\n","[INFO] processing image 28/31\n","[INFO] processing image 29/31\n","[INFO] processing image 30/31\n","[INFO] processing image 31/31\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"MU5qZWnj0t8r","executionInfo":{"status":"ok","timestamp":1616502877772,"user_tz":-180,"elapsed":4971,"user":{"displayName":"Лена","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiMK7DQoNpvJJ9b_baTOhOl-0KsCYViUs3aLi0N=s64","userId":"14206305893323667740"}}},"source":[""],"execution_count":71,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GqRKGdD1JVwh","executionInfo":{"status":"ok","timestamp":1616502877773,"user_tz":-180,"elapsed":4792,"user":{"displayName":"Лена","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiMK7DQoNpvJJ9b_baTOhOl-0KsCYViUs3aLi0N=s64","userId":"14206305893323667740"}},"outputId":"6cfc3190-ce16-4415-9c07-82676e37537e"},"source":["# dump the facial embeddings + names to disk\n","print(\"[INFO] serializing {} encodings...\".format(total))\n","data = {\"embeddings\": knownEmbeddings, \"names\": knownNames}\n","f = open('embedings.pickle', \"wb\")\n","f.write(pickle.dumps(data))\n","f.close()"],"execution_count":72,"outputs":[{"output_type":"stream","text":["[INFO] serializing 31 encodings...\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"iCtwngWz2L7S","executionInfo":{"status":"ok","timestamp":1616502878276,"user_tz":-180,"elapsed":5078,"user":{"displayName":"Лена","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiMK7DQoNpvJJ9b_baTOhOl-0KsCYViUs3aLi0N=s64","userId":"14206305893323667740"}}},"source":[""],"execution_count":72,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9TUNt1o_2Mbz"},"source":["#Step 2: Train face recognition model"]},{"cell_type":"code","metadata":{"id":"M9myJtap19Bl","executionInfo":{"status":"ok","timestamp":1616502878277,"user_tz":-180,"elapsed":4676,"user":{"displayName":"Лена","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiMK7DQoNpvJJ9b_baTOhOl-0KsCYViUs3aLi0N=s64","userId":"14206305893323667740"}}},"source":[""],"execution_count":72,"outputs":[]},{"cell_type":"code","metadata":{"id":"icDR2JhvJbew","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1616502878277,"user_tz":-180,"elapsed":4489,"user":{"displayName":"Лена","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiMK7DQoNpvJJ9b_baTOhOl-0KsCYViUs3aLi0N=s64","userId":"14206305893323667740"}},"outputId":"67d011ed-c539-41c4-e05e-c361c704c332"},"source":["print(\"[INFO] loading face embeddings...\")\n","data = pickle.loads(open('embedings.pickle', \"rb\").read())\n","# encode the labels\n","print(\"[INFO] encoding labels...\")\n","le = LabelEncoder()\n","labels = le.fit_transform(data[\"names\"])"],"execution_count":73,"outputs":[{"output_type":"stream","text":["[INFO] loading face embeddings...\n","[INFO] encoding labels...\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"oK1YgoO31r01","executionInfo":{"status":"ok","timestamp":1616502878278,"user_tz":-180,"elapsed":4274,"user":{"displayName":"Лена","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiMK7DQoNpvJJ9b_baTOhOl-0KsCYViUs3aLi0N=s64","userId":"14206305893323667740"}},"outputId":"c47f4cfb-afc6-4d40-8790-23f4d608a620"},"source":["\n","print(\"[INFO] training model...\")\n","recognizer = SVC(C=1.0, kernel=\"linear\", probability=True)\n","recognizer.fit(data[\"embeddings\"], labels)"],"execution_count":74,"outputs":[{"output_type":"stream","text":["[INFO] training model...\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["SVC(C=1.0, break_ties=False, cache_size=200, class_weight=None, coef0=0.0,\n","    decision_function_shape='ovr', degree=3, gamma='scale', kernel='linear',\n","    max_iter=-1, probability=True, random_state=None, shrinking=True, tol=0.001,\n","    verbose=False)"]},"metadata":{"tags":[]},"execution_count":74}]},{"cell_type":"code","metadata":{"id":"pyyqOYZE2ZJp","executionInfo":{"status":"ok","timestamp":1616502878278,"user_tz":-180,"elapsed":4105,"user":{"displayName":"Лена","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiMK7DQoNpvJJ9b_baTOhOl-0KsCYViUs3aLi0N=s64","userId":"14206305893323667740"}}},"source":["f = open('recognizer.pickle', \"wb\")\n","f.write(pickle.dumps(recognizer))\n","f.close()\n","# write the label encoder to disk\n","f = open('le.pickle', \"wb\")\n","f.write(pickle.dumps(le))\n","f.close()"],"execution_count":75,"outputs":[]},{"cell_type":"code","metadata":{"id":"3i3WR4nV36Pu","executionInfo":{"status":"ok","timestamp":1616502878278,"user_tz":-180,"elapsed":3912,"user":{"displayName":"Лена","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiMK7DQoNpvJJ9b_baTOhOl-0KsCYViUs3aLi0N=s64","userId":"14206305893323667740"}}},"source":[""],"execution_count":75,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"AOEHq-pk391D"},"source":["#Step 3: Recognize faces with OpenCV"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vSfM12sW3_O0","executionInfo":{"status":"ok","timestamp":1616502878279,"user_tz":-180,"elapsed":3527,"user":{"displayName":"Лена","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiMK7DQoNpvJJ9b_baTOhOl-0KsCYViUs3aLi0N=s64","userId":"14206305893323667740"}},"outputId":"bf29ee87-db2a-4a80-d0a4-a1ab3934cc8a"},"source":["print(\"[INFO] loading face detector...\")\n","# load our serialized face detector from disk\n","detector = cv2.dnn.readNetFromCaffe(\"deploy.prototxt.txt\", \"res10_300x300_ssd_iter_140000.caffemodel\")\n","# load our serialized face embedding model from disk\n","print(\"[INFO] loading face recognizer...\")\n","embedder = cv2.dnn.readNetFromTorch('openface.nn4.small2.v1.t7')\n","\n","# load the actual face recognition model along with the label encoder\n","recognizer = pickle.loads(open('recognizer.pickle', \"rb\").read())\n","le = pickle.loads(open('le.pickle', \"rb\").read())"],"execution_count":76,"outputs":[{"output_type":"stream","text":["[INFO] loading face detector...\n","[INFO] loading face recognizer...\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"output_embedded_package_id":"1HgRliY4lusUsc5GMLYOtm7g1DNIYSkZP"},"id":"di0hf8o5_JvG","executionInfo":{"status":"ok","timestamp":1616506578166,"user_tz":-180,"elapsed":23670,"user":{"displayName":"Лена","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiMK7DQoNpvJJ9b_baTOhOl-0KsCYViUs3aLi0N=s64","userId":"14206305893323667740"}},"outputId":"594df0a7-07b1-4779-feda-cd08f507c1de"},"source":["imagePaths = list(paths.list_images('test3'))\n","for (i, imagePath) in enumerate(imagePaths):\n","  image = cv2.imread(imagePath)\n","  image = imutils.resize(image, width=600)\n","  (h, w) = image.shape[:2]\n","  imageBlob = cv2.dnn.blobFromImage(cv2.resize(image, (300, 300)), 1.0, (300, 300),(104.0, 177.0, 123.0), swapRB=False, crop=False)\n","  detector.setInput(imageBlob)\n","  detections = detector.forward()\n","  for i in range(0, detections.shape[2]):\n","    confidence = detections[0, 0, i, 2]\n","    if confidence > 0.5:\n","      box = detections[0, 0, i, 3:7] * np.array([w, h, w, h])\n","      (startX, startY, endX, endY) = box.astype(\"int\")\n","      face = image[startY:endY, startX:endX]\n","      (fH, fW) = face.shape[:2]\n","      if fW < 20 or fH < 20:\n","        continue\n","      faceBlob = cv2.dnn.blobFromImage(face, 1.0 / 255, (96, 96),\n","        (0, 0, 0), swapRB=True, crop=False)\n","      embedder.setInput(faceBlob)\n","      vec = embedder.forward()\n","      preds = recognizer.predict_proba(vec)[0]\n","      j = np.argmax(preds)\n","      proba = preds[j]\n","      name = le.classes_[j]\n","      text = \"{}: {:.2f}%\".format(name, proba * 100)\n","      y = startY - 10 if startY - 10 > 10 else startY + 10\n","      cv2.rectangle(image, (startX, startY), (endX, endY),\n","        (0, 0, 255), 2)\n","      cv2.putText(image, text, (startX, y),\n","        cv2.FONT_HERSHEY_SIMPLEX, 0.45, (0, 0, 255), 2)\n","  cv2_imshow(image)\n","  cv2.waitKey(0)"],"execution_count":80,"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}]},{"cell_type":"code","metadata":{"id":"enWsW29r_kzg","executionInfo":{"status":"ok","timestamp":1616502895922,"user_tz":-180,"elapsed":20800,"user":{"displayName":"Лена","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiMK7DQoNpvJJ9b_baTOhOl-0KsCYViUs3aLi0N=s64","userId":"14206305893323667740"}}},"source":[""],"execution_count":77,"outputs":[]},{"cell_type":"code","metadata":{"id":"9mrm9_Gd-560","executionInfo":{"status":"ok","timestamp":1616502895923,"user_tz":-180,"elapsed":20588,"user":{"displayName":"Лена","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiMK7DQoNpvJJ9b_baTOhOl-0KsCYViUs3aLi0N=s64","userId":"14206305893323667740"}}},"source":[""],"execution_count":77,"outputs":[]}]}